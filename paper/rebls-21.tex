\documentclass[sigplan,screen]{acmart}

%\usepackage{listings}
\usepackage{xspace}
\newcommand{\lin}[1]{(\emph{ln. #1}\xspace)}
\newcommand{\dapp}{\emph{dapp}\xspace}
\newcommand{\dapps}{\emph{dapps}\xspace}

\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

\acmConference[REBLS '21]{REBLS '21: ACM Workshop on Reactive and Event-Based Languages and Systems}{June 03--05, 2018}{Chicago, IL}
\acmBooktitle{REBLS '21: ACM Workshop on Reactive and Event-Based Languages and Systems, June 03--05, 2018, Chicago, IL}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

%\acmSubmissionID{123-A56-BU3}
%\citestyle{acmauthoryear}

\begin{document}

\title{Symmetric Distributed Applications}

\author{Francisco Sant'Anna}
\email{francisco@ime.uerj.br}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{Rio de Janeiro State University (UERJ)}
  \country{Brazil}
}

\author{Rodrigo Santos}
\email{rodsantos@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \country{Brazil}
}

\author{Noemi Rodriguez}
\email{noemi@inf.puc-rio.br}
\affiliation{%
  \institution{PUC-Rio}
  \country{Brazil}
}

%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

\begin{abstract}
A program is deterministic if multiple re-executions with the same inputs
always lead to the same state.
Even concurrent instances of a deterministic program should observe identical
behavior---in real time---if assigned the same set of inputs.
%
In this work, we guarantee real-time reproducibility for distributed programs.
Multiple instances of the same interactive application can broadcast
asynchronous inputs and yet conform to identical behavior.
Collaborative networked applications, such as watch parties, document editing,
and video games can benefit from this approach.
We name this class of applications as \emph{symmetric distributed applications}.
%
Using a standard event-driven API to wait and emit events, programmers write
code as if the application executes in a single machine.
Our middleware intercepts event generation and synchronizes all instances so
that receipt is identically reproducible.
Not only distributed applications benefit from determinism but also development
and testing can be done in a single instance with the same guarantees.
\end{abstract}

\begin{comment}
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[100]{Networks~Network reliability}
\end{comment}

%\keywords{globally-asynchronous locally-synchronous, synchronous programming}
\keywords{TODO}

\maketitle

\section{Introduction}

Deterministic programs are easier to understand, test, and verify~\cite{det}.
Considering unpredictable user interactions, a program is deterministic if
re-execution with the same order and timing of inputs always leads to the same
state.
With such timeline reproducibility property, multiple re-executions are
indistinguishable from one another.
Considering now distribution, it should even be possible to provide the same
timeline to concurrent instances of a deterministic program and observe
identical behavior \emph{in real time}.

In this work, our goal is to guarantee the real-time reproducibility property
in a distributed setting.
Mirrored instances of the same application running in different machines can
broadcast asynchronous inputs to each other and yet conform to
identical behavior.
Hence, our focus is on \emph{symmetric distributed applications}, instead of
machines playing different roles in the network.
A programmer writes the intended distributed application as if it would execute
in a single machine.
The underlying system we propose supports distribution and guarantees a
consistent timeline to all instances.

Collaborative networked applications fall in the class of symmetric
distribution and can benefit from transparent determinism and reproducibility.
As an example, \emph{watch parties} are social gatherings to watch movies and
TV shows.
Users expect to be perfectly synchronized such that pressing the pause button
in any machine should stop all others exactly in the same video frame.
In this context, the delay imposed by the network is just an inconvenience that
should not degrade the experience in comparison to users sitting in front of
the same TV.
Other examples that fall in this category are single-screen multiplayer games
and collaborative document editing.

In addition to make distributed applications \emph{behave} like local
applications, we also intend to make distributed programs \emph{be coded} like
local programs.
In this sense, we provide a standard event-driven API with two main commands
to wait and emit events locally.
We also provide the middleware that connects multiple application instances
transparently in the network.
The middleware intercepts local event generation and synchronizes all instances
so that receipt is identically reproducible.
As a result, not only distributed applications benefit from determinism but
also development and testing can be done in a single instance with the same
guarantees.
% Specific contribs: 3 goals, scalability, async

As the main limitations, the middleware relies on a central server to determine
a total order of events, and all instances must be known and responsive during
the entire execution.
In addition, since the latency of events is proportional to the maximum
round-trip time (RTT) in the network, low-latency applications might become
impractical.
Finally, if clients diverge considerably from the expected RTT, applications
may experience intermittent freezes.
For these reasons, the proposed middleware targets soft real-time collaborative
applications.

Section~\ref{sec.arch} describes the overall architecture of our middleware.
Section~\ref{sec.sync} discusses the synchronous programming model, which
programs must comply to preserve determinism.
Section~\ref{sec.gals} discusses the globally-asynchronous locally-synchronous
architecture of our middleware, and details the synchronization protocol for
input reproducibility.
%Section~\ref{sec.async} ...
Section~\ref{sec.eval} evaluates our middleware with...
%Section~\ref{sec.apps} ... % sym video player, asy rect editor
%Section~\ref{sec.discussion}...
Section~\ref{sec.related}...
Section~\ref{sec.conclusion}...

\section{Overall Middleware Architecture}
\label{sec.arch}

Figure~\ref{fig.middleware} describes the client-server architecture of our
middleware.
A distributed application (\dapp, at the top left of the Figure), is a set of
mirrored instances running in different machines (also \dapp, at the edges of
the figure, to emphasize that they are symmetric and represent a single
application).
The clients, which are part of the middleware but co-located with each
instance, intermediate all communication with the server and enable that \dapp
instances are specified as a local application.
The server receives asynchronous events (in red) from instances and redirects
them to all clients as synchronous events with an appropriate delta delay (in
green).
The delay is necessary because network communication takes non-negligible time
(i.e., in the order of milliseconds), but instances need to advance together
to preserve reproducibility.
Hence, delaying the events is the only possibility to achieve reproducibility.
The clients control the local clock ticks in the instances and issue the
received events at the appropriate timestamps (both synchronized, in green).

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{middleware}
  \caption{
    \label{fig.middleware}
    The architecture of the middleware.
    A single server synchronizes multiple clients, each connected to a mirrored
    instance of the distributed application.
  }
  %\Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

Events represent user interactions, such as key presses, which are
unpredictable and need to be communicated with the other instances.
Since instances should be indistinguishable from each other, event sources are
irrelevant.
For instance, if a user presses a key in one instance, the \dapp behaves as if
all users pressed the same key in all instances simultaneously.
%
Clock ticks represent the rate in which applications are updated, and are
equivalent to frame rates in video playback and games.
We assume periods in the order of tens of milliseconds (e.g., 25 milliseconds
or 40 frames per second).
An important insight is that clock ticks are predictable inputs and need not to
go through the server.
This results in no delay between the client and the \dapp since interprocess
communication takes negligible time (i.e., in the order of a few microseconds).
Unlike for sporadic mechanical user inputs, users would notice and condemn
delays in clock ticks.
%
Clock ticks and delayed events constitute the unique synchronous timeline
shared by all instances of the \dapp, making them manifest identical behavior.
Figure~\ref{fig.timeline} is an example of a timeline with asynchronous events
that are synchronized with a delay.
Except for event id \emph{0}, which represent clock ticks, applications
determine their own ids.
The middleware just forwards events with no further interpretation.
As detailed in Section~\ref{sec.gals}, the middleware ensures that all
instances receive the same timeline.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
Tick    Event    Async
----    -----    -----
0000      0
0025      0      --> user presses key (1) and mouse (2)
0050      0
0075      0      --> user presses key (1)
0100      0
0125      1  <-- key is synchronized after delay
0150      0
0175      2  <-- mouse is synchronized after delay
0200      1  <-- key is synchronized after delay
0225      0
0250      0
....     ...
\end{verbatim}
}
  \caption{
    \label{fig.timeline}
    Example of a synchronized timeline of inputs shared by all instances of the
    \dapp.
    Ticks are \emph{25ms} apart (\emph{40 FPS}).
    Asynchronous events are synchronized with a delay.
    Note that delay is unpredictable but event order within each source
    instance is preserved.
  }
\end{figure}

%As detailed in Section~\ref{sec.gals}, the delta delay for user input is the
The delta delay for user input is proportional to the maximum network
round-trip time considering all clients.
We deliberately assume unbounded network delay to augment the possible
application scenarios.
Another concern is the rate of input generation in the instances.
As an example, tracking the mouse position will inevitably flood the network
with packets and make the application unresponsive.
For these reasons, the viability of applications depends on
    (i) the acceptable delay in the user input,
    (ii) the nature and rate of inputs, and
    (iii) the maximum network latency.

The code for an actual \dapp is the same for all instances and uses a standard
event-driven API with only four commands:
%
\begin{itemize}
\item \texttt{mid\_connect(port,fps)}:   \\Connects with the local client in the given port and desired FPS.
\item \texttt{mid\_disconnect()}:        \\Disconnects with the local client.
\item \texttt{(now,evt) = mid\_wait()}:  \\Waits for the next input carrying a timestamp and event id.
\item \texttt{mid\_emit(evt)}:           \\Emits the given asynchronous input.
\end{itemize}
%
Figure~\ref{fig.skel} shows the skeleton of a distributed application.
The commands \texttt{connect} and \texttt{disconnect} are only required once to
enclose the application logic.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
01  fun dapp (port: Int) {
02     mid_connect(port,40)           // connects to client at 40 FPS
03     while (true) {                 // main event loop
04        val (now,evt) = mid_wait()  // awaits input (every 25ms)
05        switch (evt) {              // reacts to input
06           ...break loop...         //   possibly terminates
07        }
08        ...mid_emit(nxt)...         // possibly generates inputs
09     }
10     mid_disconnect()               // disconnects with the client
11  }
\end{verbatim}
}
  \caption{
    \label{fig.skel}
    The skeleton of a \dapp is a main loop that waits synchronous and emits
    asynchronous events.
  }
\end{figure}

Figure~\ref{fig.timeline}~and~\ref{fig.skel} with the timeline and the program
relate as follows:
The application initially blocks waiting for an input \lin{4}.
According to the timeline, the first input happens at \emph{tick 0} with
\emph{event 0} (no event).
In the second iteration (\emph{tick 25}), the application emits events \emph{1}
and \emph{2} asynchronously \lin{8}.
Only after a few ticks, these events are synchronized (\emph{ticks 125 and 175})
in the main loop \lin{4}.
As intended, note how the main event loop is coded like a standard local
event-driven application.
We extend this discussion in the next section.

\section{Local Synchronous Programming}
\label{sec.sync}

In the synchronous programming model~\cite{sync}, a program executes in
locksteps (or logical ticks) as successive reactions to inputs provided by an
external environment.
In our context, the environment represents user interactions, and inputs can be
occasional events, such as a key press, or simply the passage of time.
Since execution is guided from outside, the main advantage of the synchronous
model is that it is possible to record a sequence of inputs and reproduce the
behavior of a program multiple times for reasoning and testing purposes.
A fundamental requirement of synchronous programming, known as the
\emph{synchronous hypothesis}~\cite{hypo}, is to isolate logical ticks from
each other to preserve locksteps and prevent concurrent reactions to inputs,
which would break determinism.
This hypothesis can be satisfied if computing reactions is faster than the rate
of external inputs.

An important concern is how to guarantee that isolated reactions are themselves
deterministic and sufficiently fast.
Synchronous languages~\cite{langs} typically restrict the programming
primitives and/or perform static analysis to ensure these properties.
However, since our solution proposes a standard event-driven API targeting
generic programming languages, we assume these properties are ensured
informally.
This may involve coding best practices, such as avoiding stateful system calls
and time-consuming loops.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{schemes}
  \caption{
    \label{fig.schemes}
    Equivalent execution schemes for synchronous systems~\cite{schemes}:
    In the first scheme, an event occurrence triggers a loop iteration.
    In the second scheme, a predefined time interval triggers a loop iteration,
    which polls the environment for events.
  }
\end{figure}

Figure~\ref{fig.schemes} shows two common implementation schemes for
synchronous programs~\cite{schemes}.
In both schemes, a loop iteration updates the state of the memory completely
before handling the next input.
Hence, assuming memory updates are deterministic, the only source of
non-determinism resides in the order of inputs from the environment.
Outputs are asynchronous events in the opposite direction of inputs and
signal the environment about changes.
They typically represent external actuators as opposed to input sensors.

The ``Sampling'' scheme of Figure~\ref{fig.schemes} is very similar to the
\dapp skeleton of Figure~\ref{fig.skel}:
    \texttt{mid\_connect} specifies the sampling period \lin{2},
    \texttt{mid\_wait} reads inputs \lin{4},
    \texttt{mid\_emit} signals the environment back \lin{8}, and
    the \texttt{switch} statement processes the inputs to update memory \lin{5--7}.
As required by the synchronous hypothesis, the sampling period, and
consequently the rate of inputs, must be compatible with the processing speed.
A subtle particularity is that \dapps outputs are actually asynchronous inputs
later retrofitted back synchronized with a delay, as described in
Figure~\ref{fig.middleware}.
%
The sampling scheme is also adopted by popular event-driven libraries, such as
\emph{SDL} for computer graphics, and \emph{Arduino} for embedded systems.%
\footnote{\url{www.libsdl.org} and \url{www.arduino.cc}}
This allows for an easier integration of our middleware with practical systems,
and also reinforces how, in our proposal, programming distributed versions
becomes similar to their local counterparts.

Figure~\ref{fig.sdl} is the code for a \dapp written in SDL to control an
animated rectangle on the screen with the keyboard.%
\footnote {
    Video with two distributed instances: \url{https://youtu.be/HSIwmrUenqg}
}
The code structure follows the synchronous implementation scheme with well
delimited regions to initialize memory \lin{4--8}, read inputs in a loop
\lin{11--13}, update memory \lin{15--28}, and compute outputs \lin{30--31}.
As mentioned above, the outputs are actually asynchronous inputs that are later
retrofitted into the \dapp.
The output region is expanded in Figure~\ref{fig.input} and is discussed in
sequence.
The application first initializes the SDL library to create a window and
renderer handle \lin{4--6}.
The rectangle starts at position \texttt{(10,10)} with no movement in the axes:
\texttt{vx=vy=0} \lin{7--8}.
The main loop \lin{10--32} first waits for the next input to control the
rectangle \lin{11--13}.
On each iteration, the screen is cleared and the rectangle is drawn on the
current position \lin{15--19}.
The \texttt{switch} statement \lin{20--27} processes the current input:
    a clock tick \lin{21} just moves the rectangle in the current direction;
    a space \lin{22} pauses the rectangle by resetting the axes speeds;
    the arrow keys \lin{23--26} sets the axis speeds towards the appropriate direction.
Before the next iteration, the screen is updated \lin{28}.
The code is enclosed by middleware calls to connect and disconnect \lin{2,34}.
Inside the main loop, all state modifications depend only on the received
event, which ensures that the application is deterministic.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
01  int main (void) {
02    mid_connect(port, 40); // 25ms ticks
03
04    SDL_Init(SDL_INIT_VIDEO);                 ---\
05    SDL_Window*   win = SDL_CreateWindow();      |
06    SDL_Renderer* ren = SDL_CreateRenderer(win); |> Initialize
07    int x=10, vx=0; // position and              |    Memory
08    int y=10, vy=0; // speed multipler        ---/
09
10    while (1) {
11      uint64_t now;                           ---\
12      uint32_t evt;                              |> Read Inputs
13      mid_read(&now, &evt);                   ---/
14
15      SDL_SetRenderDrawColor(ren, WHITE);     ---\
16      SDL_RenderClear(ren); // clear screen      |
17      SDL_Rect r = { x, y, 10, 10 };             |
18      SDL_SetRenderDrawColor(ren, RED);          |
19      SDL_RenderFillRect(ren, &r); // draw rect  |
20      switch (evt) { // 5px/40fps -> 200 px/s    |
21        case 0:     { x+=5*vx; y+=5*vy; break; } |> Update Memory
22        case SPACE: { vy= 0; vx=0; break; }      |
23        case LEFT:  { vx=-1; vy=0; break; }      |
24        case RIGHT: { vx= 1; vy=0; break; }      |
25        case UP:    { vy=-1; vx=0; break; }      |
26        case DOWN:  { vy= 1; vx=0; break; }      |
27      }                                          |
28      SDL_RenderPresent(ren);                 ---/
29
30      // emit asynchronous inputs             ---\  Compute Outputs
31      ... mid_emit(evt) ...                   ---/
32    }
33
34    mid_disconnect();
35    return 0;
36  }
\end{verbatim}
}
  \caption{
    \label{fig.sdl}
    A \dapp in SDL to control an animated rectangle on the screen with the keyboard.
  }
\end{figure}

Figure~\ref{fig.input} expands the output region with the calls to
\texttt{mid\_emit} that generate asynchronous inputs.
In this application, the code simply calls the SDL library to check if a key is
pressed to forward it to the middleware.
Note that in a local-only application this region of code would replace the
region to read inputs from the middleware.
Note also that it is not necessary to customize this code for every single
application.
Instead, the middleware may provide a custom input generation stub for each
event-driven library it supports, which \dapps can reuse.
This is the reason why we split this code in a separate figure.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
// emit asynchronous inputs
SDL_Event inp;
if (SDL_PollEvent(&inp)) {
   if (inp.type == SDL_KEYDOWN) {
      switch (inp.key.keysym.sym) {
         case SDLK_LEFT:  { mid_emit(LEFT);  break; }
         case SDLK_RIGHT: { mid_emit(RIGHT); break; }
         case SDLK_UP:    { mid_emit(UP);    break; }
         case SDLK_DOWN:  { mid_emit(DOWN);  break; }
         case SDLK_SPACE: { mid_emit(SPACE); break; }
      }
   }
}
\end{verbatim}
}
  \caption{
    \label{fig.input}
    Asynchronous input generation with \texttt{mid\_emit}.
    The library polls for key presses and forwards them to the middleware.
  }
\end{figure}

To conclude this section, the ``Update Memory'' region of Figure~\ref{fig.sdl}
\lin{15--28}, which contains the core logic of the application, does not make
calls to the
middleware, being indistinguishable from a local version.
In addition, since this region complies with the synchronous model premises,
the application remains responsive and deterministic.
For instance, the computations to update the positions are clearly faster than
\emph{25ms} (a tick period), satisfying the synchronous hypothesis.
Also, if a timeline such as that of Figure~\ref{fig.timeline} is applied to
multiple re-executions of the application, the sequence of memory updates to
\texttt{x}, \texttt{y}, \texttt{vx}, and \texttt{vy} \lin{20--27} will be
identical every single time.
In the next section, we discuss how to extend these guarantees to a distributed
setting.

\section{Distributed GALS Architecture}
\label{sec.gals}

The synchronous programming model assumes a single clock that controls the main
loop to update the application state in locksteps.
However, when we distribute the \dapp in multiple instances, we introduce a
delay due to communication, and we can no longer assume a single clock source,
which compromises the synchronous hypothesis.

Towards symmetric distributed applications with identical behavior, we need to
read inputs synchronized and in real time in all instances.
For instance, in the rectangle example of Figure~\ref{fig.sdl}, the challenge
is
    to awake calls to \texttt{mid\_read} at the same time in all instances \lin{13}, and
    to broadcast and then resynchronize all asynchronous calls to \texttt{mid\_emit} \lin{31}.

The ``Globally-Asynchronous Locally-Synchronous Architecture (GALS)''
integrates multiple independent synchronous processes as a single distributed
application~\cite{gals.taxonomy}.
%
Inspired by the GALS architecture, we can keep the simplicity and guarantees of
the synchronous model to program local instances, and transfer the
responsibility to deal with asynchrony to a middleware at the global level.
%
In Figure~\ref{fig.middleware}, we propose a GALS-like architecture.
We can see that \dapp local instances depend on synchronous inputs only (green
arrows), and are allowed to output asynchronous events (red arrows).
%
Unlike traditional GALS, the middleware automatically resynchronizes the
inputs in a single global timeline such that all instances observe them
identically.

As introduced in Section~\ref{sec.arch}, resynchronization adds a delta delay
to the original event proportional to the maximum network round-trip time
considering all clients.
The reason is because the synchronization algorithm needs to consider two main
issues:
%
\begin{itemize}
\item \textbf{Clock differences:}
    Each instance has an independent clock that differs from others in
    \emph{offset} and \emph{rate}.
    The offset refers to the point in time since the \dapp started.
    As an example, suppose one instance started \emph{5000ms} ago while another
    started \emph{5051ms} ago.
    The \emph{rate} refers to how fast the clock progresses, which may
    slightly differ between machines.
    As an example, suppose one instance misses \emph{25ms} every hour in
    comparison to others.
    This difference, aka \emph{clock drift}, may affect the time offset
    considerably over time.
\item \textbf{Network delay:}
    Instances are subject to a communication delay with the server, which
    varies between clients and also over time.
    As an example, suppose a client next to the server has a consistent RTT of
    \emph{10ms}, while a distant one may vary between \emph{50--100ms} over
    time.
\end{itemize}
%
We do not assume any strict timing bounds in our synchronization algorithm,
which may of course affect the viability of some applications.

Overall, the synchronization algorithm performed by the middleware has the
following goals:
%
\begin{enumerate}
\item Keeping instances clock offsets close to each other.
\item Generating synchronous inputs back to instances with minimum delays.
\item Ensuring that instances read inputs with perfect time accuracy.
\end{enumerate}
%
Regarding \emph{goal 1}, offset differences are inevitable but should be under
a few milliseconds to be unnoticed by users.
Regarding \emph{goal 2}, the smaller is the input delay, the better is the user
experience regarding interactivity.
However, if too small, a distant instance may nevertheless receive the delayed
input after its local time, which is unacceptable considering \emph{goal 3}.
In this case, the middleware may freeze the instance and pause its local time
to ensure time accuracy.

Figure~\ref{fig.protocol} describes the synchronization protocol performed by
the middleware.%
\footnote {
    Video with two distributed instances, now with deliberate RTTs in the order
    of \emph{100ms}: \url{https://youtu.be/bf9C2mwkTzA}
}
The next paragraphs explains the protocol in detail.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{protocol}
  \caption{
    \label{fig.protocol}
    The synchronization protocol
        (i) keeps clock offsets together,
        (ii) generates synchronous inputs with minimum delays, and
        (iii) ensures that instances read inputs with accuracy.
  }
  %\Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

The server starts first and needs to know in advance how many instances will
participate in the \dapp (\emph{Server (N)}).
Each instance connects through its client with the server
(\emph{Client 1 ... Client N}), which in turn waits for all connections to
succeed (\emph{all connected}).
Then, the server broadcasts a \emph{start} message to all clients, which on
receipt, starts sending clock ticks to their instances (\emph{started}).
Note that clients will inevitably start at different absolute times due to
network latency.
The clients also send back an \emph{ack} so that the server can calculate the
maximum network RTT (\emph{RTT1=45ms}), which is used next.
%
After the startup process, each \dapp instance executes at the same pace
(except for drifts) with similar offset differences (at most \emph{RTT1}).
We consider that, at this point, the instances are indistinguishable from each
other.

Now, let's consider an asynchronous event (in red): \emph{Client N} emits event
\emph{1} at local time \emph{750ms} (\emph{async(750,1)}).
The server enqueues all event requests and handles each one atomically, in
sequence, as described further.
%
It is important to recognize that a client cannot by itself determine a
reasonable timestamp for an event and broadcast it directly to others.
First, the client has no idea about the network RTT to propose a deadline
timestamp that guarantees that all clients receive the event in time to fulfill
\emph{goal 3} (e.g, \emph{750+RTT}).
Second, even if known, there is no guarantee that the RTT will not slightly
increase momentarily, making an instance to miss the event deadline.
Third, even if the RTT was known and bounded, local instances might still
diverge on local time offsets (e.g., \emph{750ms} vs \emph{800ms}), again
making an instance ahead to miss the deadline.
%
As mentioned above, we do not assume any bounds in RTT and offset variance.

Hence, our protocol relies on the server to determine a reasonable timestamp
for all clients, and to ensure that instances do not miss deadlines even if
everything goes wrong.
%
First, the server broadcasts the intention to emit an event
(\emph{propose(750,45)}), passing the source instance offset time
(\emph{750ms}) and current overall network RTT (\emph{RTT1=45ms}).
The final synchronous event (\emph{sync(895,1)}) is yet to come after all
clients acknowledge the server.
Before that, each client calculates its own deadline as follows:
    (i)   take the maximum time between the source and local offset,
    (ii)  add twice the received RTT,
    (iii) add a small constant time (\emph{5ms}),
    (iv)  set this sum as the local event deadline.
As examples, the sum for
    \emph{Client 1} is $max(750,700)+2x45+5=845$, and for
    \emph{Client N} is $max(750,800)+2x45+5=895$.
%
This sum is reasonable because it considers
    the instance most ahead in time, and also
    the worst RTT with an extra margin.
For instance, \emph{Client N} with the worst RTT (\emph{45ms}) is also ahead of
time (\emph{800ms}) and the final event will arrive after another round trip to
the server, thus making \emph{895ms} a reasonable deadline.
%
The small constant depends on the number of instances in the application and is
currently set to 200us per instance (e.g., \emph{5ms} for \emph{25} instances).
%
During this process, the server also recalculates the RTT to use in the next
event emission (\emph{RTT2=50ms}).

Each client also starts a timer to freeze the instance if the (now expected)
synchronous event is not received until the deadline.
In this case, the client pauses the instance by repeating the same clock tick
over and over, until it receives the event.
Although unfortunate, this at least ensures identical timelines in all
instances of the \dapp, ensuring \emph{goal 3}.
%
The server then waits for all client \emph{acks} carrying their local offsets
(\emph{ack(700)} and \emph{ack(800)}), recalculates each deadline (in the same
way each client did), and takes the largest as the final event timestamp to
broadcast to all clients (\emph{sync(895,1)}).
The server also sends drift compensations (\emph{dt(100)} and \emph{dt(0)})
based on offset differences between clients, which is discussed further.
At this point, all clients are expecting a timestamp that is greater than or
equal to their own timer deadlines.
Therefore, even if the network deteriorates, no instances will miss the
synchronized event because they will be frozen before its deadline.

Finally, the expected synchronous event is received by the clients
(\emph{sync(895,1)}).
\emph{Client N} receives it before the deadline, at local time \emph{870ms},
which requires to postpone local emission for another \emph{25ms} to match the
event timestamp with perfect accuracy (\emph{895ms}).
However, \emph{Client 1} expires its timer before receipt and needs to freeze
the \dapp for a while (red area).
Then, the synchronous event is received at local time \emph{860ms}, but still
requires another \emph{35ms} to match its timestamp.
In the end, both clients emit the event exactly at local time \emph{895ms} as
expected (white circles).

Note that \emph{Client 1} was harmed in the process mainly because its maximum
time of \emph{750ms} was much smaller than the \emph{800ms} \emph{Client N}
had.
This is because the local clocks were too distant from each other, breaking our
\emph{goal 1}.
As an additional remark, note that pausing \emph{Client 1}, increased the
offset difference even further (\emph{offset diff}).
%
The middleware also implements an algorithm to compensate clock drifts as
follows:%
\footnote {
    Video with two distributed instances, now with exaggerated clock drifts:
    \url{https://youtu.be/wwlAURjN5YY}
}
Once the server receives all acknowledges carrying local offsets
(\emph{ack(700)} and \emph{ack(800)}), the server sends back how much each
client is delayed in comparison to the maximum value (i.e., \emph{800-700=100}
and \emph{800-800=0}).
If this number is greater than zero, the client speeds up each local tick in
\emph{20\%} until the time drift is compensated.
For instance, if the client behind generates new ticks every \emph{25ms}, it
will instead generate \emph{25ms} ticks every \emph{20ms}.

In order to maintain the overall network RTT updated and the clock offsets
close to each other, the first client to connect with the server sends a
periodic null event (\emph{id=0}) every \emph{5 seconds} to kick off the
synchronization algorithm automatically%
\footnote {
    Video with two distributed instances, now with random RTTs, clock drifts,
    and periodic synchronization:
    \url{https://youtu.be/BYNHGsHnMx8}
}.

In the next section, we evaluate the scalability of the protocol%
\footnote {
    Video with 100 distributed instances running in the localhost:
    \url{https://youtu.be/in5sdEEyGik}
}.

\section{Evaluation}
\label{sec.eval}

We present the protocol evaluation experiments taking into consideration its
main goals:
%
\begin{enumerate}
\item Keeping instances clock offsets close to each other.
\item Generating synchronous inputs back to instances with minimum delays.
\item Ensuring that instances read inputs with perfect time accuracy.
\end{enumerate}
%
In order to verify if these goals are met, we measure 4 properties as follows:
%
\begin{enumerate}
\item \textbf{Clock drift}:
    the percentage of drift compensation relative to the total simulation time
    (e.g., $1\%$, if drifts accumulate $1s$ in $100s$).
\item \textbf{Event latency}:
    the average number of frames elapsed from asynchronous emits to their
    corresponding synchronous receipts (e.g., $3$, if an emit occurs at frame
    $58$ and receipt at $61$).
\item \textbf{Frame freeze}:
    the percentage of repeated frames relative to the total simulation frames
    (e.g., $1\%$, if $1$ frame freezes every $100$ frames).
\item \textbf{Frame late}:
    the percentage of frame delays due to CPU load relative to the total
    simulation time (e.g., $1\%$, if delays accumulate $1s$ in $100s$).
\end{enumerate}
%
Drift compensation ensures that the offsets are kept closer, but a high
percentage of clock drift may indicate that the protocol is troublesome.
Ideally, clock drift rate should be $0\%$.
%
A high event latency may compromise \emph{goal 2}, but a low latency may
increase the amount of frame freezes, which is even less desirable.
Ideally, event latency should be $1$ frame, and freeze rate, $0\%$.
%
As an implicit goal, the applications preserve their original frame rates when
going distributed.
The protocol should not affect this property (and vice versa).
Ideally, frame late rate should be $0\%$.
However, in our experiments, we simulate a CPU load between $50--110\%$ to
stress the protocol.
Hence, ideally, late rate should be around $TODO\%$.
%
Note that \emph{goal 3} is a strict requirement in our design, which the
protocol ensures with 100\% accuracy.
In this sense, our experiments logs the timelines of all instance and just
asserts that they are identical in all simulations, since this is the only
possible result.

Our experiments simulates a variety of scenarios by parameterizing the
following variables:
%
\begin{enumerate}
\item Number of instances in the network (\emph{N}):
        affects network traffic and server CPU load.
\item Network type (\emph{NET}):
        affects round-trip time (RTT).
\item The application frame rate (\emph{FPS}):
        affects clients CPU load.
\item The application rate of input generation (\emph{EVT}):
        affects network traffic and server CPU load.
\end{enumerate}
%
For instance, for a video game with a few friends over Ethernet, the parameters
would be
    \emph{3--5} instances,
    an RTT of \emph{5--10ms},
    FPS around \emph{50},
    and a new interaction around every \emph{500ms}.
An interactive remote class over the internet could reach
    \emph{100} instances,
    an RTT of \emph{200ms},
    a lower FPS of \emph{25},
    and an interaction around every \emph{5s} (e.g., a slide advance).
%
For each of these four dimensions (\emph{N}, \emph{NET}, \emph{FPS}, and
\emph{EVT}), we tested the following values:
%
\begin{itemize}
\item \emph{N}: $2$, $5$, $10$, $50$, and $100$ instances.                  % 5
\item \emph{NET}: \emph{Localhost}, \emph{Ethernet}, and \emph{Internet} types
                  of network (with RTTs in the range of \emph{0--200ms}).   % 3
\item \emph{FPS}: $10$, $25$, $50$, $100$ frames per second.                % 4
\item \emph{EVT}: $5s$, $1s$, $500ms$, and $250ms$ period between each event.
\end{itemize}                                                               % 4
%
This values combined result in $240$ combinations.
Each combination executes for $5$ minutes and is repeated $3$ times.
The experiments take $60$ hours to complete.

Overall, the results show...

\begin{comment}
- RTT (para ver se bate com estilo da rede)
- Latencia/Delay (entre async/sync)
    - O minimo seria 2RTT+DT/2 por conta do protocolo
    - O minimo teorico seria RTT/2 em media, que é o tempo de bcast de um evento
- Relacao entre RTT e latencia

% - what ifs c/ videos
% - revisitar limitacoes e propor alternativas na secao de avaliacao
% - Need total order in real time. Ok leader, raft, etc, not ok Blockchain
\end{comment}

\section{Asymmetric Distributed Applications}

\begin{itemize}
\item precisa de nao determinismo para mudar comportamento
\item nao determinismo se resuma a uma unica nova API: gals-self-nondet()  <-- retorna ID local
\item assimetria se reduz em "if (gals-self-nondet() == xxx) ..."
\item cuidados para os ifs apenas serem para geração de eventos (controle 1, controle 2, etc) e view local (TV, controle, etc)
\end{itemize}

\begin{comment}
With distribution, communication timing is asynchronous because communication
latency takes a non-negligible time and breaks the synchronous hypothesis.
What about in realtime
- a perfect mirror, cannot distinguish
- high-level vs low-leve events, semantic events
    - more abstract solves both problems (delay and rate)
- clear/sound properties to reason
- Events: single application, multiple views, may restrict events per node
    - gals\_self()

Concurrent programs in non-deterministic languages are notoriously hard to prove correct and have led to well-known disasters.
\end{comment}

\begin{comment}
\section{Applications}
\label{sec.apps}

\subsection{Symmetric Video Playback}

\begin{itemize}
\item mesmo video e interface em todas as instancias
\item clique de "pause" tem efeito no mesmo frame de todas as instancias
\item stub do gstreamer controlado frame a frame
\end{itemize}

\subsection{Asymmetric Graphics Editor}

\begin{itemize}
\item editor retangulos (cria, redimensiona, move)
\item dois cursores "assimétricos"
\item precisa de eventos de alto nivel (provavelmente nao dá pra manter a posicao dos mouses em tempo real)
\end{itemize}
\end{comment}

\section{Related Work}
\label{sec.related}

Synchronous languages~\cite{langs} target real-time reactive systems and focus
on static guarantees for critical control systems.
They are based on a well-behaved lockstep execution that simplifies reasoning
and verification of programs.
They also restrict control and data primitives to enforce determinism and
static bounds on memory usage and execution time.
On the one hand, these guarantees are important to preserve identical behavior
to local instances in symmetric distributed application.
On the other hand, the restrictive semantics of synchronous languages hinders
the development of rich collaborative applications.
We chose a pragmatic approach and provide a standard event-driven API with no
restrictions.
Nonetheless, this API may be used as host environments in synchronous
languages.

In GALS architectures~\cite{gals.taxonomy}, computations within individual
synchronous nodes are deterministic, with the communication latency as the only
source of non-determinism.
Some programming languages and systems~\cite{gals.crp,gals.crsm,gals.systemj}
expose this architecture to programmers, which remain with the responsibility
to coordinate interprocess communication.
%
Concurrent Reactive Processes (CRP)~\cite{gals.crp} extend the synchronous
programming language Esterel~\cite{esterel} with asynchronous communication
channels resembling CSP~\cite{csp}.
%
Esterel enforces determinism and bounded reactions to local instances, which
are linked by asynchronous non-deterministic communication channels.
Instances are asymmetric in the sense that they may execute different
applications.
Also, there is no central coordination towards a global timeline, and each
instance advances its clock independently.
%
CRSM~\cite{gals.crsm} and SystemJ~\cite{gals.systemj} are similar systems, with
CSP-like asynchronous communication and Esterel-like synchronous lockstep
execution.
The main difference to CRP is that they use Argos~\cite{argos} (CRSM) and Java
(SystemJ) as their local programming languages.

``Physically Asynchronous Logically Synchronous Systems (PALS)''~\cite{pals} is
a synchronization protocol to support strict real time distributed
applications.
PALS provides a perfectly synchronized global clock shared by all instances,
which leads to identical state transitions in all instances.
On the one hand, it provides stronger synchronicity in comparison to our
protocol, in which local clocks advance independently and events need to be
delayed to fit all instances timelines, possibly resulting in application
freezes.
On the other hand, PALS also needs to rely on stronger assumptions, namely
bounded limits for clock skews, network transmission, and local computation
time.
In our protocol, allowing unbounded limits only affects the viability of
applications in certain scenarios.
Our protocol also keeps the instances clocks close to each other, but not as a
strict real-time constraint.
This keeps the instances indistinguishable for the human eyes and prevents the
clock distances to degrade input delay predictions.
%
Regarding the protocol implementation, PALS also relies on a central server to
serialize the timeline.
However, since the bounds are fixed and known in advance and all instances are
at the same logical tick, PALS can skip the ``proposal round'' required by our
protocol, which decreases the latency of inputs.

%- even 100fps under some circs, event inet

\begin{comment}
Asynchronous languages and models:

Derflow: distributed deterministic dataflow programming for erlang
Erlang implements a message-passing execution model in which concurrent processes send each other messages asynchronously. This model is inherently non-deterministic: a process can receive messages sent by any process which knows its process identifier, leading to an exponential number of possible executions based on the number messages received.
We propose a new execution model for Erlang, ''Deterministic Dataflow Programming'', based on a highly available, scalable single-assignment data store implemented on top of the riak\_core distributed systems framework.

Deterministic Actors
While actors provide a more disciplined model for concurrency than threads, their interactions, if not constrained, admit nondeterminism.
 We describe “reactors,” a new coordination model that combines ideas from several of the aforementioned approaches to enable determinism while preserving much of the style of actors.

Deterministic replay of distributed Java applications
Execution behavior of a Java application can be nondeterministic due to concurrent threads of execution, thread scheduling, and variable network delays. This nondeterminism in Java makes the understanding and debugging of multi-threaded distributed Java applications a difficult and a laborious process.
It is well accepted that providing deterministic replay of application execution is a key step towards programmer productivity and program under-standing.
Towards this goal, we developed a replay framework based on logical thread schedules and logical intervals.

Synchronous languages and models:

A Programming Model for Time-Synchronized Distributed Real-Time Systems
Discrete-event (DE) models are formal system specifications that have analysable deterministic behaviors. Using a global, consistent notion of time, DE components communicate via time-stamped events.
In this paper, we extend DE models with the capability of relating certain events to physical time.
Our technique relies on having a distributed common notion of time, known to some precision.
\end{comment}

\section{Conclusion}
\label{sec.conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{rebls-21}

\end{document}
\endinput
