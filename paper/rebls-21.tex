\documentclass[sigplan,screen]{acmart}

%\hyphenation{globally-asyn-chronous}

%\usepackage{listings}
\usepackage{xspace}
\newcommand{\lin}[1]{(\emph{ln. #1}\xspace)}
\newcommand{\dapp}{\emph{dapp}\xspace}
\newcommand{\dapps}{\emph{dapps}\xspace}

\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

\acmConference[REBLS '21]{REBLS '21: ACM Workshop on Reactive and Event-Based Languages and Systems}{June 03--05, 2018}{Chicago, IL}
\acmBooktitle{REBLS '21: ACM Workshop on Reactive and Event-Based Languages and Systems, June 03--05, 2018, Chicago, IL}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

%\acmSubmissionID{123-A56-BU3}
%\citestyle{acmauthoryear}

\begin{document}

\title{Symmetric Distributed Applications}

\author{Francisco Sant'Anna}
\email{francisco@ime.uerj.br}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{Rio de Janeiro State University (UERJ)}
  \country{Brazil}
}

\author{Rodrigo Santos}
\email{rodsantos@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \country{Brazil}
}

\author{Noemi Rodriguez}
\email{noemi@inf.puc-rio.br}
\affiliation{%
  \institution{PUC-Rio}
  \country{Brazil}
}

%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

\begin{abstract}
A program is deterministic if multiple re-executions with the same inputs
always lead to the same state.
Even concurrent instances of a deterministic program should observe identical
behavior---in real time---if assigned the same set of inputs.
%
In this work, we propose real-time reproducibility for distributed programs.
Multiple instances of the same interactive application can broadcast
asynchronous inputs and yet conform to identical behavior.
Collaborative networked applications, such as watch parties, document editing,
and video games can benefit from this approach.
We name this class of applications as \emph{symmetric distributed applications}.
%
Using a standard event-driven API to wait and emit events, programmers write
code as if the application executes in a single machine.
Our middleware intercepts event generation and synchronizes all instances so
that receipt is identically reproducible.
Not only distributed applications benefit from determinism but also development
and testing can be done in a single instance with the same guarantees.
The middleware can handle applications with 25 FPS, distributed in 25 nodes
over the Internet, with an event latency below $350ms$.
\end{abstract}

\begin{comment}
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[100]{Networks~Network reliability}
\end{comment}

\keywords{GALS, synchronous programming, distributed applications}

\maketitle

\section{Introduction}

Deterministic programs are easier to understand, test, and verify~\cite{det}.
Considering unpredictable user interactions, a program is deterministic if
re-execution with the same order and timing of inputs always leads to the same
state.
With such timeline reproducibility property, multiple re-executions are
indistinguishable from each other.
Considering now distribution, it should even be possible to provide the same
timeline to concurrent instances of a deterministic program and observe
identical behavior \emph{in real time}.

In this work, our goal is to support the real-time reproducibility property in
a distributed setting.
Mirrored instances of the same application running in different machines can
broadcast asynchronous inputs to each other and yet conform to
identical behavior.
Hence, our focus is on \emph{symmetric distributed applications}, instead of
machines playing different roles in the network.
A programmer writes the intended distributed application as if it would execute
in a single machine.
The underlying system we propose supports distribution and guarantees a
consistent timeline to all instances.

Collaborative networked applications fall in the class of symmetric
distribution and can benefit from transparent determinism and reproducibility.
As an example, \emph{watch parties} are social gatherings to watch movies and
TV shows.
Users expect to be perfectly synchronized such that pressing the pause button
in any machine should stop all others exactly in the same video frame.
In this context, the delay imposed by the network is just an inconvenience that
should not degrade the experience in comparison to users sitting in front of
the same TV.
Other examples that fall in this category are single-screen multiplayer games
and collaborative document editing.

In addition to make distributed applications \emph{behave} like local
applications, we also intend to make distributed programs \emph{be coded} like
local programs.
In this sense, we provide a standard event-driven API with two main commands
to wait and emit events locally.
We also provide the middleware that connects multiple application instances
transparently in the network.
The middleware intercepts local event generation and synchronizes all instances
so that receipt is identically reproducible.
As a result, not only distributed applications benefit from determinism but
also development and testing can be done in a single instance with the same
guarantees.
The middleware can handle applications with 25 FPS, distributed in 25 nodes
over the Internet, with an event latency below $350ms$.

As the main limitations, the middleware relies on a central server to determine
a total order of events, and all instances must be known and responsive during
the entire execution.
In addition, since the latency of events is proportional to the maximum
round-trip time (RTT) in the network, low-latency applications might become
impractical.
Finally, if clients diverge considerably from the expected RTT, applications
may experience intermittent freezes.
For these reasons, the proposed middleware targets soft real-time collaborative
applications.

Section~\ref{sec.arch} describes the overall architecture of our middleware.
Section~\ref{sec.sync} discusses the synchronous programming model, which
programs must comply to preserve determinism.
Section~\ref{sec.gals} discusses how we adapted the globally-asynchronous
locally-synchronous architecture with a synchronization protocol for input
reproducibility.
%Section~\ref{sec.async} ...
Section~\ref{sec.eval} evaluates the performance and scalability of our
middleware in multiple network and application scenarios.
%Section~\ref{sec.apps} ... % sym video player, asy rect editor
%Section~\ref{sec.discussion}...
Section~\ref{sec.related} compares our proposal with related work.
Section~\ref{sec.conclusion} concludes this paper.

\section{Overall Middleware Architecture}
\label{sec.arch}

Figure~\ref{fig.middleware} describes the client-server architecture of our
middleware.
A distributed application (\dapp, at the top left of the Figure), is a set of
mirrored instances running in different machines (also \dapp, at the edges of
the figure, to emphasize that they are symmetric and represent a single
application).
The clients are part of the middleware, but are co-located with each running
\dapp instance with very low communication latency.
They intermediate all communication with the server and enable that instances
are specified as local applications.
As the figure illustrates, the latency between the instances and respective
clients is in the order of microseconds, while the latency between clients and
the server is in the order of milliseconds.
The server receives asynchronous events (in red) from instances and redirects
them to all clients as synchronous events with an appropriate delta delay on
their timestamps (in green).
The clients also control the local clock ticks in the instances and issue the
received events at the appropriate timestamps (both synchronized, in green).
Note that the clock ticks are not transmitted, nor synchronized by the server.
%
The timestamps represent the exact times in which all instances must apply the
events to their identical local timelines.
The delays in the events are necessary because of latency in network
communications.
Without delays, instances would inevitably receive events too late to apply,
with local times already ahead of event timestamps.


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{middleware}
  \caption{
    \label{fig.middleware}
    The architecture of the middleware.
    A single server synchronizes multiple clients, each connected to a mirrored
    instance of the distributed application.
  }
  %\Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

Events represent user interactions, such as key presses, which are
unpredictable and need to be communicated with the other instances.
Since instances must be indistinguishable from each other by design, event
sources are irrelevant and are ignored.
For instance, if a user presses a key in one instance, the \dapp behaves as if
all users pressed the same key in all instances simultaneously.
%
Clock ticks represent the rate in which applications advance, and are
equivalent to frame rates in video playback and games.
%We assume periods in the order of tens of milliseconds (e.g., 25 milliseconds
%or 40 frames per second).
An important insight is that clock ticks are predictable inputs and need not to
go through the server.
This results in no delay between the client and the \dapp since local
interprocess communication takes negligible time.
% (i.e., in the order of a few microseconds).
Otherwise, unlike delays in sporadic mechanical user inputs, users would notice
and condemn delays in clock ticks.
%
Clock ticks and delayed events constitute the unique synchronous timeline
shared by all instances of the \dapp, making them manifest identical behavior.
Figure~\ref{fig.timeline} is an example of a timeline with asynchronous events
that are synchronized with a delay.
Except for id \emph{0}, which represents clock ticks, applications determine
their event ids, which the middleware just forward with no further
interpretation.
As detailed in Section~\ref{sec.gals}, the middleware ensures that all
instances receive the same timeline.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
Tick    Event    Async
----    -----    -----
0000      0
0025      0      --> user presses key (1) and mouse (2)
0050      0
0075      0      --> user presses key (1)
0100      0
0125      1  <-- key is synchronized after delay
0150      0
0175      2  <-- mouse is synchronized after delay
0200      1  <-- key is synchronized after delay
0225      0
0250      0
....     ...
\end{verbatim}
}
  \caption{
    \label{fig.timeline}
    Example of a synchronized timeline of inputs shared by all instances of the
    \dapp.
    Ticks are \emph{25ms} apart (\emph{40 FPS}).
    Asynchronous events are synchronized with a delay.
    Note that delay is unpredictable but event order within each source
    instance is preserved.
  }
\end{figure}

%As detailed in Section~\ref{sec.gals}, the delta delay for user input is the
The delta delay for user input is proportional to the maximum RTT considering
all clients.
We deliberately assume unbounded network delay to augment the possible
application scenarios.
Another concern is the rate of input generation in the instances.
As an example, tracking the mouse position will inevitably flood the network
with packets and make applications unresponsive.
For these reasons, the viability of applications depends on
    (i) the acceptable delay in the user input,
    (ii) the nature and rate of inputs, and
    (iii) the maximum network latency.

The code for an actual \dapp is the same for all instances and uses a standard
event-driven API with only four commands:
%
\begin{itemize}
\item \texttt{mid\_connect(port,fps)}:   \\Connects with the local client in the given port and desired FPS.
\item \texttt{mid\_disconnect()}:        \\Disconnects with the local client.
\item \texttt{(now,evt) = mid\_wait()}:  \\Waits for the next input carrying a timestamp and event id.
\item \texttt{mid\_emit(evt)}:           \\Emits the given asynchronous input.
\end{itemize}
%
Figure~\ref{fig.skel} shows the skeleton of a distributed application.
The commands \texttt{connect} and \texttt{disconnect} are only required once to
enclose the application logic.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
01  fun dapp (port: Int) {
02     mid_connect(port,40)           // connects to client at 40 FPS
03     while (true) {                 // main event loop
04        val (now,evt) = mid_wait()  // awaits input (every 25ms)
05        switch (evt) {              // reacts to input
06           ...break loop...         //   possibly terminates
07        }
08        ...mid_emit(nxt)...         // possibly generates inputs
09     }
10     mid_disconnect()               // disconnects with the client
11  }
\end{verbatim}
}
  \caption{
    \label{fig.skel}
    The skeleton of a \dapp is a main loop that waits synchronous and emits
    asynchronous events.
  }
\end{figure}

Figure~\ref{fig.timeline} with the timeline and \ref{fig.skel} with the program
relate as follows:
The application initially blocks waiting for an input \lin{4}.
According to the timeline, the first input happens at \emph{tick 0} with
\emph{event 0} (no event).
In the second iteration (\emph{tick 25}), the application emits events \emph{1}
and \emph{2} asynchronously \lin{8}.
Only after a few ticks, these events are synchronized (\emph{ticks 125 and 175})
in the main loop \lin{4}.
As intended, note how the main event loop is coded like a standard local
event-driven application.
We extend this discussion in the next section.

\section{Local Synchronous Programming}
\label{sec.sync}

In the synchronous programming model~\cite{sync}, a program executes in
locksteps (or logical ticks) as successive reactions to inputs provided by an
external environment.
In our context, the environment represents user interactions, and inputs can be
occasional events, such as a key press, or simply the passage of time.
Since execution is guided from outside, the main advantage of the synchronous
model is that it is possible to record a sequence of inputs and reproduce the
behavior of a program multiple times for reasoning and testing purposes.
A fundamental requirement of synchronous programming, known as the
\emph{synchronous hypothesis}~\cite{hypo}, is to isolate logical ticks from
each other to preserve locksteps and prevent concurrent reactions to inputs,
which would break determinism.
This hypothesis can be satisfied if computing reactions is faster than the rate
of external inputs.

An important concern is how to guarantee that isolated reactions are themselves
deterministic and sufficiently fast.
Synchronous languages~\cite{langs} typically restrict the programming
primitives and/or perform static analysis to ensure these properties.
However, since we propose a standard event-driven API targeting generic
programming languages, we assume informally that the programmers themselves
ensure these properties.
This may involve coding best practices, such as avoiding stateful system calls
and time-consuming loops in programs.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{schemes}
  \caption{
    \label{fig.schemes}
    Equivalent execution schemes for synchronous systems~\cite{schemes}:
    In the first scheme, an event occurrence triggers a loop iteration.
    In the second scheme, a predefined time interval triggers a loop iteration,
    which polls the environment for events.
  }
\end{figure}

Figure~\ref{fig.schemes} shows two common implementation schemes for
synchronous programs~\cite{schemes}.
In both schemes, a loop iteration updates the state of the memory completely
before handling the next input.
Hence, assuming that memory updates are deterministic, the only source of
non-determinism resides in the order of inputs from the environment.
Outputs are asynchronous events in the opposite direction of inputs and
signal the environment about changes.
They typically represent external actuators as opposed to input sensors.

The ``Sampling'' scheme of Figure~\ref{fig.schemes} is very similar to the
\dapp skeleton of Figure~\ref{fig.skel}:
    \texttt{mid\_connect} specifies the sampling period \lin{2},
    \texttt{mid\_wait} reads inputs \lin{4},
    \texttt{mid\_emit} signals the environment back \lin{8}, and
    the \texttt{switch} statement processes the inputs to update memory \lin{5--7}.
As required by the synchronous hypothesis, the sampling period, and
consequently the rate of inputs, must be compatible with the processing speed
of the loop body.
A particularity of our design is that \dapps outputs are actually asynchronous
inputs retrofitted back later synchronized with a delay, as described in
Figure~\ref{fig.middleware}.
%
The sampling scheme of Figure~\ref{fig.schemes} is also adopted by popular
event-driven libraries, such as \emph{SDL} for computer graphics, and
\emph{Arduino} for embedded systems.%
\footnote{SDL: \url{www.libsdl.org}. Arduino: \url{www.arduino.cc}.}
This allows for an easier integration of our middleware with practical systems,
and also reinforces how, in our proposal, programming distributed versions
becomes similar to their local counterparts.

Figure~\ref{fig.sdl} is the code for a \dapp written in SDL to control an
animated rectangle on the screen with the keyboard.%
\footnote {
    Video with two distributed instances: \url{https://youtu.be/HSIwmrUenqg}
}
The code structure follows the synchronous scheme with well delimited regions
to initialize memory \lin{4--8}, read inputs in a loop \lin{11--13}, update
memory \lin{15--28}, and compute outputs \lin{30--31}.
As mentioned above, the outputs are actually asynchronous inputs that are later
retrofitted into the \dapp.
The output region is expanded in Figure~\ref{fig.input} and is discussed in
sequence.
The application first initializes the SDL library to create a window and
renderer handle \lin{4--6}.
The rectangle starts at position \texttt{(10,10)} with no movement in the axes:
\texttt{vx=vy=0} \lin{7--8}.
The main loop \lin{10--32} first waits for the next input to control the
rectangle \lin{11--13}.
On each iteration, the screen is cleared and the rectangle is drawn on the
current position \lin{15--19}.
The \texttt{switch} statement \lin{20--27} processes the current input:
    a clock tick \lin{21} just moves the rectangle in the current direction;
    a space \lin{22} pauses the rectangle by resetting the axes speeds;
    the arrow keys \lin{23--26} set the axis speeds towards the appropriate direction.
Before the next iteration, the screen is updated \lin{28}.
The code is enclosed by middleware calls to connect and disconnect \lin{2,34}.
Inside the main loop, all state modifications are sequential and depend only on
the received event, which ensures that the application is responsive and
deterministic.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
01  int main (void) {
02    mid_connect(port, 40); // 25ms ticks
03
04    SDL_Init(SDL_INIT_VIDEO);                 ---\
05    SDL_Window*   win = SDL_CreateWindow();      |
06    SDL_Renderer* ren = SDL_CreateRenderer(win); |> Initialize
07    int x=10, vx=0; // position and              |    Memory
08    int y=10, vy=0; // speed multipler        ---/
09
10    while (1) {
11      uint64_t now;                           ---\
12      uint32_t evt;                              |> Read Inputs
13      mid_read(&now, &evt);                   ---/
14
15      SDL_SetRenderDrawColor(ren, WHITE);     ---\
16      SDL_RenderClear(ren); // clear screen      |
17      SDL_Rect r = { x, y, 10, 10 };             |
18      SDL_SetRenderDrawColor(ren, RED);          |
19      SDL_RenderFillRect(ren, &r); // draw rect  |
20      switch (evt) { // 5px/40fps -> 200 px/s    |
21        case 0:     { x+=5*vx; y+=5*vy; break; } |> Update Memory
22        case SPACE: { vy= 0; vx=0; break; }      |
23        case LEFT:  { vx=-1; vy=0; break; }      |
24        case RIGHT: { vx= 1; vy=0; break; }      |
25        case UP:    { vy=-1; vx=0; break; }      |
26        case DOWN:  { vy= 1; vx=0; break; }      |
27      }                                          |
28      SDL_RenderPresent(ren);                 ---/
29
30      // emit asynchronous inputs             ---\  Compute Outputs
31      ... mid_emit(evt) ...                   ---/
32    }
33
34    mid_disconnect();
35    return 0;
36  }
\end{verbatim}
}
  \caption{
    \label{fig.sdl}
    A \dapp in SDL to control an animated rectangle on the screen with the keyboard.
  }
\end{figure}

Figure~\ref{fig.input} expands the output region with calls to
\texttt{mid\_emit} that generate asynchronous inputs.
In this application, the code simply calls the SDL library to check if a key is
pressed to forward it to the middleware \lin{11--13}.
Note that in a local-only application this region of code would replace the
region to read inputs from the middleware.
Note also that it is not necessary to customize this code for every single
\dapp.
Instead, the middleware may provide a custom input generation stub for each
event-driven library it supports, which \dapps can reuse.
This is the reason why we split this code in a separate figure.

\begin{figure}[t]
{\scriptsize
\begin{verbatim}
// emit asynchronous inputs
SDL_Event inp;
if (SDL_PollEvent(&inp)) {
   if (inp.type == SDL_KEYDOWN) {
      switch (inp.key.keysym.sym) {
         case SDLK_LEFT:  { mid_emit(LEFT);  break; }
         case SDLK_RIGHT: { mid_emit(RIGHT); break; }
         case SDLK_UP:    { mid_emit(UP);    break; }
         case SDLK_DOWN:  { mid_emit(DOWN);  break; }
         case SDLK_SPACE: { mid_emit(SPACE); break; }
      }
   }
}
\end{verbatim}
}
  \caption{
    \label{fig.input}
    Asynchronous input generation with \texttt{mid\_emit}.
    The library polls for key presses and forwards them to the middleware.
  }
\end{figure}

To conclude this section, the ``Update Memory'' region of Figure~\ref{fig.sdl}
\lin{15--28}, which contains the core logic of the application, does not make
calls to the
middleware, being indistinguishable from a local version.
In addition, since this region complies with the synchronous model premises,
the application remains responsive and deterministic.
For instance, the computations to update the positions are clearly faster than
\emph{25ms} (a tick period), satisfying the synchronous hypothesis.
Also, if a timeline such as that of Figure~\ref{fig.timeline} is applied to
multiple re-executions of the application, then the sequence of memory updates
to \texttt{x}, \texttt{y}, \texttt{vx}, and \texttt{vy} \lin{20--27} will be
identical in every single execution.
In the next section, we discuss how to extend these guarantees to a distributed
setting.

\section{Distributed GALS Architecture}
\label{sec.gals}

The synchronous programming model assumes a single clock that controls the main
loop to update the application state in locksteps.
However, when we distribute the \dapp in multiple instances, we can no longer
assume a single clock source, which compromises the synchronous hypothesis.
% we introduce a delay due to communication, and
%
Nevertheless, towards our goal of symmetric distributed applications with
identical behavior, we need to read inputs perfectly synchronized and in real
time in all instances.
For instance, in the rectangle example of Figure~\ref{fig.sdl}, the challenge
is
    to awake calls to \texttt{mid\_read} at the same time in all instances \lin{13}, and
    to broadcast and then resynchronize all asynchronous calls to \texttt{mid\_emit} \lin{31}.

The ``Globally-Asynchronous Locally-Synchronous Architecture (GALS)''
integrates multiple independent synchronous processes as a single distributed
application~\cite{gals.taxonomy}.
%
Inspired by GALS, we not only want to keep the simplicity and guarantees of the
synchronous model locally, but also transfer the responsibility to deal with
asynchrony to the middleware globally.
%
As illustrated in Figure~\ref{fig.middleware}, \dapp local instances depend on
synchronous inputs only (green arrows), and are allowed to output asynchronous
events (red arrows).
%
However, unlike traditional GALS, the middleware automatically resynchronizes
inputs in a single global timeline such that all instances observe them
identically.

As introduced in Section~\ref{sec.arch}, resynchronization adds a delta delay
to the original event that is proportional to the maximum RTT considering all
clients.
The reason is because the synchronization protocol needs to consider two main
issues:
%
\begin{itemize}
\item \textbf{Clock differences:}
    Each instance has an independent clock that differs from others in
    \emph{offset} and \emph{rate}.
    The offset corresponds to the absolute point in time since the \dapp started.
    As an example, suppose one instance started \emph{5000ms} ago while another
    started \emph{5051ms} ago.
    The \emph{rate} corresponds to how fast the clock progresses, which may
    slightly differ between machines.
    As an example, suppose one instance misses \emph{25ms} every hour in
    comparison to others.
    This difference, aka \emph{clock drift}, may affect the time offset
    considerably over time.
\item \textbf{Network delay:}
    Instances are subject to communication delays with the server, which
    varies between clients and also over time.
    As an example, suppose a client next to the server has a consistent RTT of
    \emph{10ms}, while a distant one may vary between \emph{50--100ms} over
    time.
\end{itemize}
%
In our synchronization protocol, we do not assume any strict bounds in clock
differences and network delays, which may of course affect the viability of
some applications.
%
The protocol has the following goals:
%
\begin{enumerate}
\item Keeping instances clock offsets close to each other.
\item Generating synchronous inputs back to instances with minimum delays.
\item Ensuring that instances read inputs with perfect time accuracy.
\end{enumerate}
%
Regarding \emph{goal 1}, offset differences are inevitable but should be under
a few milliseconds to be unnoticed by users.
Regarding \emph{goal 2}, the smaller is the input delay, the better is the user
experience regarding interactivity.
However, if too small, a distant instance may nevertheless receive the delayed
input after its local time, which is unacceptable considering \emph{goal 3}.
In this case, the middleware may freeze the instance and pause its local time
to ensure time accuracy.

Figure~\ref{fig.protocol} describes the synchronization protocol performed by
the middleware.%
\footnote {
    Video with two distributed instances, now with deliberate RTTs in the order
    of \emph{100ms}: \url{https://youtu.be/bf9C2mwkTzA}
}
The next paragraphs explains the protocol in detail.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{protocol}
  \caption{
    \label{fig.protocol}
    The synchronization protocol
        (i) keeps clock offsets together,
        (ii) generates synchronous inputs with minimum delays, and
        (iii) ensures that instances read inputs with accuracy.
  }
  %\Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

The server starts first and needs to know in advance how many instances will
participate in the \dapp (\emph{Server (N)}).
Each instance connects through its client (\emph{Client 1 ... Client N}) with
the server, which in turn waits for all connections to succeed
(\emph{all connected}).
Then, the server broadcasts a \emph{start} message to all clients, which on
receipt, starts sending clock ticks to their instances (\emph{started}).
Note that clients will inevitably start at different absolute times due to
network latency.
The clients also send back an \emph{ack} so that the server can calculate the
maximum network RTT (\emph{RTT1=45ms}), which is used next.
%
After the startup process, each \dapp instance executes at the same pace
(except for drifts) with similar offset differences (at most \emph{RTT1}).
At this point, we consider instances to be indistinguishable from each other.

Now, let's consider an asynchronous event (in red): \emph{Client N} emits event
\emph{1} at local time \emph{750ms} (\emph{async(750,1)}).
The server enqueues all requests and handles each event atomically, in
sequence, as described further.
%
It is important to recognize that a client cannot by itself determine a
reasonable timestamp for an event and broadcast it directly to others.
First, the client has no idea about the network RTT to guarantee that all
clients would receive the event in time to fulfill \emph{goal 3}
(e.g, \emph{750+RTT}).
Second, even if known, there is no guarantee that the RTT will not slightly
increase momentarily, making an instance to miss the event deadline.
Third, even if the RTT was known and bounded, local instances might still
diverge on local time offsets (e.g., \emph{750ms} vs \emph{800ms}), again
making an instance ahead to miss the deadline.
%
As mentioned above, we do not assume any bounds in RTT and offset variance.

Hence, our protocol relies on the server to determine a reasonable timestamp
for all clients, and to ensure that instances do not miss deadlines even if
everything goes wrong.
%
First, the server broadcasts the intention to emit an event
(\emph{propose(750,45)}), passing the source instance offset time
(\emph{750ms}) and current overall RTT (\emph{RTT1=45ms}).
The final synchronous event (\emph{sync(895,1)}) is yet to come after all
clients acknowledge the server.
Before that, each client calculates its own deadline as follows:
    (i)   take the maximum time between the source and local offset,
    (ii)  add twice the received RTT,
    (iii) add a small constant time (\emph{5ms}),
    (iv)  set this sum as the local event deadline.
As examples, the sum for
    \emph{Client 1} is $max(750,700)+2x45+5=845$, and for
    \emph{Client N} is $max(750,800)+2x45+5=895$.
%
This sum is reasonable because it takes into account
    the instance most ahead in time, and also
    the worst RTT with an extra margin.
For instance, \emph{Client N} with the worst RTT (\emph{45ms}) is also ahead of
time (\emph{800ms}) and the final event will arrive after another round trip to
the server, thus making \emph{895ms} a reasonable deadline.
%
The small constant depends on the number of instances in the application and is
currently set to 200us per instance (e.g., \emph{5ms} for \emph{25} instances).
%
During this process, the server also recalculates the RTT to use in the next
event emission (\emph{RTT2=50ms}).

Each client also starts a timer to freeze the instance if the (now expected)
synchronous event is not received until the deadline.
In this case, the client pauses the instance by repeating the same clock tick
over and over, until it receives the event.
Although unfortunate, this at least ensures identical timelines in all
instances to comply with \emph{goal 3}.
%
The server then waits for all client \emph{acks} carrying their local offsets
(\emph{ack(700)} and \emph{ack(800)}), recalculates each deadline (in the same
way each client did), and takes the largest as the final event timestamp to
broadcast to all clients (\emph{sync(895,1)}).
The server also sends drift compensations (\emph{dt(100)} and \emph{dt(0)})
based on offset differences between clients, which is discussed further.
At this point, all clients are expecting a timestamp that is greater than or
equal to their own timer deadlines.
Therefore, even if the network deteriorates, no instances will miss the
synchronized event because they will be frozen before their deadlines.

Finally, the expected synchronous event is received by the clients
(\emph{sync(895,1)}).
\emph{Client N} receives it before the deadline, at local time \emph{870ms},
which requires to postpone local emission for another \emph{25ms} to match the
event timestamp with perfect accuracy (\emph{895ms}).
However, \emph{Client 1} expires its timer before receipt and needs to freeze
the \dapp for a while (red area).
Then, the synchronous event is received at local time \emph{860ms}, but still
requires another \emph{35ms} to match its timestamp.
In the end, both clients emit the event exactly at local time \emph{895ms} as
expected (white circles).

Note that \emph{Client 1} was harmed in the process mainly because its maximum
time of \emph{750ms} was much smaller than the \emph{800ms} \emph{Client N}
had.
This is because the local clocks were too distant from each other, breaking our
\emph{goal 1}.
As an additional remark, note that pausing \emph{Client 1}, increased the
offset difference even further (\emph{offset diff}).
%
The middleware also implements an algorithm to compensate clock drifts as
follows:%
\footnote {
    Video with two distributed instances, now with exaggerated clock drifts:
    \url{https://youtu.be/wwlAURjN5YY}
}
Once the server receives all acknowledges carrying local offsets
(\emph{ack(700)} and \emph{ack(800)}), the server sends back how much each
client is delayed in comparison to the maximum value (i.e., \emph{800-700=100}
and \emph{800-800=0}).
If this number is greater than zero, the client speeds up each local tick in
\emph{20\%} until the time drift is compensated.
For instance, if the client behind generates new ticks every \emph{25ms}, it
will instead generate \emph{25ms} ticks every \emph{20ms}.

In order to maintain the overall network RTT updated and the clock offsets
close to each other, the first client to connect with the server sends a
periodic null event (\emph{id=0}) every \emph{5 seconds} to kick off the
synchronization protocol automatically%
\footnote {
    Video with two distributed instances, now with random RTTs, clock drifts,
    and periodic synchronization:
    \url{https://youtu.be/BYNHGsHnMx8}
}.

In the next section, we evaluate the performance of the protocol%
\footnote {
    Video with 100 distributed instances running in the Localhost:
    \url{https://youtu.be/in5sdEEyGik}
}.

\section{Evaluation}
\label{sec.eval}

We present the protocol evaluation taking into consideration its main goals:
%
\begin{enumerate}
\item Keeping instances clock offsets close to each other.
\item Generating synchronous inputs back to instances with minimum delays.
\item Ensuring that instances read inputs with perfect time accuracy.
\end{enumerate}
%
In order to verify if these goals are met, we created a series of experiments
to measure the properties as follows:
%
\begin{itemize}
\item \textbf{Clock drift}:
    the percentage of drift compensation relative to the total simulation time
    (i.e., $1\%$, if drifts accumulate $1s$ in $100s$).
\item \textbf{Event latency}:
    the average number of frames elapsed from asynchronous emits to their
    corresponding synchronous receipts (e.g., $3$, if an emit occurs at frame
    $58$ and receipt at $61$).
\item \textbf{Frame freeze}:
    the percentage of repeated frames relative to the total simulation frames
    (i.e., $1\%$, if $1$ frame freezes every $100$ frames).
\begin{comment}
\item \textbf{Frame late}:
    the percentage of frame delays due to CPU load relative to the total
    simulation time (i.e., $1\%$, if delays accumulate $1s$ in $100s$).
\end{comment}
\end{itemize}
%
Drift compensation ensures that clock offsets are kept closer (\emph{goal~1}),
but a high percentage of clock drift may indicate that the protocol is causing it.
Ideally, clock drift rate should be $0\%$.
%
A high event latency may compromise \emph{goal 2}, but a low latency may
increase the amount of frame freezes, which is even less desirable.
Ideally, event latency should be $1$ frame, and freeze rate, $0\%$.
%
\begin{comment}
As an implicit goal, the applications preserve their original frame rates when
going distributed.
The protocol should not affect this property (and vice versa).
Ideally, frame late rate should be $0\%$.
However, in our experiments, we simulate a CPU load between \emph{50--110\%} to
stress the protocol.
%Hence, ideally, late rate should be around $TODO\%$.
\end{comment}
%
Note that \emph{goal 3} is a strict requirement, which the protocol ensures
with 100\% accuracy by design.
In this sense, our experiments log the timelines of all instances and just
asserts that they are identical in all simulations, since this is the only
possible result.

We simulated a variety of scenarios by parameterizing the following variables:
%
\begin{itemize}
\item \emph{NET} (network connection type):
    affects network RTT.
\item \emph{FPS} (application frame rate):
    affects clients CPU load.
\item \emph{N} (number of instances in the network):
    affects network traffic and server CPU load.
\item \emph{Rate} (rate of inputs):
    affects network traffic and client/server CPU load.
\end{itemize}
%
\begin{comment}
For instance, for a video game with a few friends over the LAN, the parameters
could be
    \emph{3--5} instances,
    an RTT of \emph{5--10ms},
    FPS around \emph{50},
    and a new interaction around every \emph{500ms}.
\end{comment}
As an example of parameters in a concrete scenario, an interactive class over
the Internet could have
    an RTT of \emph{50--200ms} (\emph{NET}),
    execute at \emph{25} frames per second (\emph{FPS}),
    with \emph{50} instances (\emph{N}),
    interacting every \emph{5s} (\emph{Rate}).
%e.g., a slide advance).

For each of these four dimensions, we tested the following values:
%
\begin{itemize}
\item \emph{NET}: \emph{Loopback}, \emph{LAN}, and \emph{WAN} (\emph{0--50ms} RTT).
                                                                 % 3
\item \emph{FPS}: $10$, $25$, $50$, $100$ frames per second.     % 4
\item \emph{N}: $2$, $5$, $10$, $25$, $50$, and $100$ instances. % 6
\item \emph{Rate}: $5s$, $1s$, $500ms$, and $250ms$ period between events.
\end{itemize}                                                    % 4
%
This values combined result in $288$ combinations.
We executed each combination $3$ times for $5$ minutes each.
The experiments take $72$ hours to complete.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{table2}
  \caption{
    \label{fig.table}
The \emph{Loopback}, \emph{LAN}, and \emph{WAN} regions each contains $36$
measures for multiple configurations of $FPS$, $N$, and $Rate$.
The rate of inputs appears in \emph{seconds per events}, e.g., $0.50$ means two
events per second.
%
The color thresholds (\emph{yellow},\emph{red}) vary between WAN and local
networks (LAN/Loopback) and are as follows:
    \emph{Latency} (Local: 2.5,5 ; WAN: 5,10);
    \emph{Drift}   (Local: 0.5,1 ; WAN: 1,3);
    \emph{Freeze}  (Local: 0.5,1 ; WAN: 1,3).
%
The \emph{Loopback} experiments used a single machine with $1$ process for the
server, $N$ processes for the clients, and $N$ processes for the \dapp
instances.
The \emph{LAN} and \emph{WAN} experiments used a machine dedicated for the
server, and $3$ other machines each with $N/3$ processes for the clients and
instances.
%
The experiments use heterogeneous \emph{Linux} distributions with the machine
configurations as follows:
    \emph{Loopback} (\emph{i7/8GB});
    \emph{LAN} (server \emph{i7/8GB}; clients \emph{i7/16GB}, \emph{i7/8GB}, \emph{i5/4GB});
    \emph{WAN} (server \emph{Xeon-VM/1GB}; clients \emph{2x i7/8GB}, \emph{i5/4GB}).
  }
  %\Description{A woman and a girl in white dresses sit in an open car.}
\end{figure*}

Figure~\ref{fig.table} shows a summary with $108$ out of all $288$ measures,
which are enough to draw the conclusions to follow.
We omit \emph{N=2}, \emph{N=100}, \emph{FPS=100}, and \emph{Rate=5s} to save
space in the figure.
The color of the squares indicates their performances
    (white=good, yellow=moderate, and red=poor).

The thin vertical rectangle labelled \textbf{1} in the figure highlights a
experiment running at $25$ FPS, with $25$ simultaneous instances, generating an
event every second.
%
The results show that this scenario is viable in all network configurations,
including WAN:
%
The latency around $8$ frames means that an instance broadcasts an asynchronous
event and applies it synchronized $330ms$ later, in the average.
The drift of $2.6\%$ means that for every minute of execution in each instance,
the middleware needs to compensate $1500ms$.
The freezed frames of $0.9\%$ means that for every $2400$ frames ($1$ minute),
$20$ frames are repeated.
%Late frames correspond to $0.67\%$, i.e., $16$ frames in $1$ minute.
%
Unsurprisingly, the results for local networks with the same parameters are
better, specially latency and drift.

Rectangle \textbf{2} indicates a safe region for all network types: up to $10$
FPS, $10$ instances, and $4$ events per second.
%
Rectangle \textbf{3} indicates a safe region for LANs: up to $25$ FPS, $50$
instances, and $4$ events per second.
%
The worst measured averages in these two large regions are
    a $2.7$ latency in the LAN ($110ms$),
    a $2.9\%$ drift and
    a $0.4\%$ freeze over the WAN.
%
Therefore, we can conclude that low frame-rate applications ($10$ FPS), with
reasonable network size ($10$ instances) and high activity ($4$ events per
second) can be reliable even over the WAN.
For LANs, the reliability reaches high frame rates ($50$ FPS) and a larger
network size ($25$ instances), which is enough even for video games in a local
network.

Rectangles \textbf{4} indicate a region of interest for WANs: up to $50$ FPS
and $10$ nodes.
The worst measured averages in these rectangles are
    a $12.1$ latency ($500ms$),
    a $5.8\%$ drift, and
    a $1.5\%$ freeze.
This is a high frame rate region that admits a reasonable number of nodes,
possibly with low event latency, which is enough for applications such as
family watch parties over the Internet.

The other squares in the figure should be considered in isolation and depend on
the target application and its requirements.
%
As an example, let's consider rectangle \textbf{5}: $50$ FPS, $50$ instances,
and $4$ events per second for the WAN.
A $21.7$ latency corresponds to almost $1$ second, but which might still be a
reasonable delay for many applications.
A drift of $9.8$ means that the server will be constantly, but smoothly,
compensating the instances clocks.
A freeze below $2\%$ over the Internet is also a reasonable value.

\begin{comment}
- RTT (para ver se bate com estilo da rede)
- Latencia/Delay (entre async/sync)
    - O minimo seria 2RTT+DT/2 por conta do protocolo
    - O minimo teorico seria RTT/2 em media, que é o tempo de bcast de um evento
- Relacao entre RTT e latencia

% - what ifs c/ videos
% - revisitar limitacoes e propor alternativas na secao de avaliacao
% - Need total order in real time. Ok leader, raft, etc, not ok Blockchain
\end{comment}

\begin{comment}
\section{Asymmetric Distributed Applications}

\begin{itemize}
\item precisa de nao determinismo para mudar comportamento
\item nao determinismo se resuma a uma unica nova API: gals-self-nondet()  <-- retorna ID local
\item assimetria se reduz em "if (gals-self-nondet() == xxx) ..."
\item cuidados para os ifs apenas serem para geração de eventos (controle 1, controle 2, etc) e view local (TV, controle, etc)
\end{itemize}

With distribution, communication timing is asynchronous because communication
latency takes a non-negligible time and breaks the synchronous hypothesis.
What about in realtime
- a perfect mirror, cannot distinguish
- high-level vs low-leve events, semantic events
    - more abstract solves both problems (delay and rate)
- clear/sound properties to reason
- Events: single application, multiple views, may restrict events per node
    - gals\_self()

Concurrent programs in non-deterministic languages are notoriously hard to prove correct and have led to well-known disasters.

\section{Applications}
\label{sec.apps}

\subsection{Symmetric Video Playback}

\begin{itemize}
\item mesmo video e interface em todas as instancias
\item clique de "pause" tem efeito no mesmo frame de todas as instancias
\item stub do gstreamer controlado frame a frame
\end{itemize}

\subsection{Asymmetric Graphics Editor}

\begin{itemize}
\item editor retangulos (cria, redimensiona, move)
\item dois cursores "assimétricos"
\item precisa de eventos de alto nivel (provavelmente nao dá pra manter a posicao dos mouses em tempo real)
\end{itemize}
\end{comment}

\section{Related Work}
\label{sec.related}

Synchronous languages~\cite{langs} focus on static guarantees for critical
real-time control systems.
They are based on a well-behaved lockstep execution that simplifies reasoning
and verification of programs.
They also restrict control and data primitives to enforce determinism and
static bounds on memory usage and execution time.
On the one hand, these guarantees enforce identical behavior to local instances
in symmetric distributed application.
On the other hand, the restrictive semantics of synchronous languages hinders
the development of rich collaborative applications.
We chose a pragmatic approach and provide a standard event-driven API with no
restrictions.
Nonetheless, this API can support host environments in synchronous languages.

In GALS architectures~\cite{gals.taxonomy}, computations within local
synchronous nodes are deterministic, with communication latency being the only
global source of nondeterminism.
Some programming languages and systems~\cite{gals.crp,gals.crsm,gals.systemj}
expose this architecture to programmers, which remain with the responsibility
to coordinate interprocess communication over the network.
%
Concurrent Reactive Processes (CRP)~\cite{gals.crp} extend the synchronous
language Esterel~\cite{esterel} with asynchronous communication
channels resembling CSP~\cite{csp}.
%
Esterel enforces determinism and bounded reactions to local instances, which
are linked by asynchronous channels.
Instances are asymmetric in the sense that they may execute different
applications.
Also, there is no central coordination towards a global timeline, and each
instance advances its clock independently.
%
CRSM~\cite{gals.crsm} and SystemJ~\cite{gals.systemj} are similar systems, with
CSP-like asynchronous channels and Esterel-like synchronous lockstep execution.
The main difference to CRP is that they use Argos~\cite{argos} (CRSM) and Java
(SystemJ) as their local programming languages.

``Physically Asynchronous Logically Synchronous Systems (PALS)''~\cite{gals.pals}
is a synchronization protocol to support strict real-time distributed
applications.
PALS provides a perfectly synchronized global clock shared by all instances,
which leads to identical state transitions in all instances.
%
On the one hand, PALS provides stronger synchronicity in comparison to our
protocol.
In our proposal, local clocks advance independently and events need to be
delayed to fit all instances timelines, possibly resulting in application
freezes.
%
On the other hand, PALS requires stronger assumptions, namely bounded limits
for clock skews, network latency, and local computation time.
We support unbounded limits that only affect the viability of applications in
certain scenarios.
%
Our protocol also keeps the instances clocks close to each other, but not as a
strict real-time constraint.
This is enough to keep the instances indistinguishable for the human eyes, and
not affect the prediction of input delays.
%
Regarding the protocol implementation, PALS also relies on a central server to
serialize the timeline, but delivers a lower event latency.
Since the bounds are fixed and known in advance, and all instances are
at the same logical tick, PALS can skip the ``proposal round'' required by our
protocol.

\begin{comment}
Asynchronous languages and models:

Derflow: distributed deterministic dataflow programming for erlang
Erlang implements a message-passing execution model in which concurrent processes send each other messages asynchronously. This model is inherently non-deterministic: a process can receive messages sent by any process which knows its process identifier, leading to an exponential number of possible executions based on the number messages received.
We propose a new execution model for Erlang, ''Deterministic Dataflow Programming'', based on a highly available, scalable single-assignment data store implemented on top of the riak\_core distributed systems framework.

Deterministic Actors
While actors provide a more disciplined model for concurrency than threads, their interactions, if not constrained, admit nondeterminism.
 We describe “reactors,” a new coordination model that combines ideas from several of the aforementioned approaches to enable determinism while preserving much of the style of actors.

Deterministic replay of distributed Java applications
Execution behavior of a Java application can be nondeterministic due to concurrent threads of execution, thread scheduling, and variable network delays. This nondeterminism in Java makes the understanding and debugging of multi-threaded distributed Java applications a difficult and a laborious process.
It is well accepted that providing deterministic replay of application execution is a key step towards programmer productivity and program under-standing.
Towards this goal, we developed a replay framework based on logical thread schedules and logical intervals.

Synchronous languages and models:

A Programming Model for Time-Synchronized Distributed Real-Time Systems
Discrete-event (DE) models are formal system specifications that have analysable deterministic behaviors. Using a global, consistent notion of time, DE components communicate via time-stamped events.
In this paper, we extend DE models with the capability of relating certain events to physical time.
Our technique relies on having a distributed common notion of time, known to some precision.
\end{comment}

\section{Conclusion}
\label{sec.conclusion}

In this work, we propose \emph{Symmetric Distributed Applications} as mirrored
instances running in multiple machines that can broadcast inputs to each other
and yet conform to identical behavior.
%
Using a standard event-driven API, programmers write code as if the application
executes in a single machine.
Our middleware intercepts event generation and synchronizes all instances with
a delay such that receipt is identically reproducible.
%
Not only distributed applications benefit from determinism but also development
and testing can be done in a single instance with the same guarantees.

The middleware is based on a GALS architecture, in which nodes are locally
deterministic, but subject to a global source of nondeterminism due to network
latency.
However, unlike traditional GALS, our middleware automatically resynchronizes
inputs in a single global timeline such that all instances observe them
identically.
%The middleware needs to handle clock differences between instances as well as
%latency variations in the network.
%The middleware keeps the instance clocks close to each other, generates
The synchronization protocol works in two phases when receiving an asynchronous
input:
First, it asks the clients for a reasonable deadline, which also prepare them
for the exact incoming timestamp.
Second, the middleware collects all client responses and sends the synchronized
event back to clients.

We evaluate the performance of the protocol with a series of experiments
varying the network latency and size, and the application frame and event rate.
%
We highlighted a reliable and high-performant region of parameters of up to
$10$ FPS and $10$ instances for WANs, and of up to $25$ FPS and $50$ instances
for LANs.
We also identified a region of interest for WANs of up to $50$ FPS and $10$
nodes, but with an event latency in the order of $500ms$.
%
The protocol evaluation helps identifying the scenarios in which the middleware
can handle hypothetical distributed applications.

\bibliographystyle{ACM-Reference-Format}
\bibliography{rebls-21}

\end{document}
\endinput
